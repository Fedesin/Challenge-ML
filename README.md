# ğŸ’» Challenge-ML
## ğŸ“Š Proyecto: AnÃ¡lisis de Dataset de Contratos

### ğŸ“ DescripciÃ³n

Este proyecto tiene como objetivo analizar el dataset **CUAD_v1** proporcionado para el challenge. El dataset contiene informaciÃ³n de contratos legales en diferentes formatos y estÃ¡ organizado en varias subcarpetas con documentos en PDF, TXT y datos estructurados en CSV y JSON. El propÃ³sito es aplicar un proceso ETL para extraer, transformar y analizar la informaciÃ³n, generando reportes y conclusiones Ãºtiles a partir de los datos.

### ğŸ¯ Objetivo del Proyecto

- Realizar un anÃ¡lisis exploratorio del dataset para identificar patrones y obtener insights Ãºtiles.
- Responder preguntas clave relacionadas con los contratos de los proveedores, tales como:
  - ğŸ“Œ Â¿CuÃ¡l es el proveedor con mÃ¡s contratos asociados?
  - ğŸ“Œ Â¿CuÃ¡l es el plazo de renovaciÃ³n de contrato mÃ¡s breve? Â¿Y el mÃ¡s extenso?
  - ğŸ“Œ Â¿CuÃ¡l es la fecha de inicio de contrato mÃ¡s antigua?
  - ğŸ“Œ Â¿CuÃ¡l es el paÃ­s asociado a la ley aplicable? Â¿A quÃ© continente pertenece?

Implementar el proceso ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) para preparar los datos para el anÃ¡lisis.
Generar grÃ¡ficos y reportes que ayuden a visualizar las respuestas a las preguntas clave.

### ğŸ”„ Proceso ETL Implementado

El proceso ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) se ha implementado de la siguiente manera:

1. **ExtracciÃ³n:** Descarga y carga del dataset desde Kaggle utilizando la API de Kaggle.
2. **TransformaciÃ³n:** Limpieza y estructuraciÃ³n de los datos en un DataFrame de pandas.
3. **Carga:** Almacenamiento de los datos transformados en un CSV limpio (`dataset_limpio.csv`), listo para su anÃ¡lisis.


## ğŸ“¥ Proceso para descargar el dataset y aplicarlo en el colab
1. Obtener el API Token de Kaggle desde el perfil de Kaggle, lo cual generarÃ¡ el archivo `kaggle.json`.
2. Colocar el archivo `kaggle.json` en una carpeta de Google Drive.
3. Crear un notebook en Google Colab e iniciar con los siguientes pasos:
```
! pip install kaggle
```
```
from google.colab import drive
drive.mount('/content/drive')
```
Creamos para mayor comodidad a la hora de trabajar una carpeta oculta .kaggle en el directorio raiz
```
!mkdir ~/.kaggle
```
En donde dice "Challenge ML" ahÃ­ deben poner el nombre de la carpeta donde guardaron el API Token de Kaggle
```
! cp /content/drive/MyDrive/"Challenge ML"/Kaggle_API/kaggle.json ~/.kaggle/
```
```
! chmod 600 ~/.kaggle/kaggle.json
```
Ahora con el siguiente comando vamos a descargar el dataset dentro de nuestro google colab
```
! kaggle datasets download konradb/atticus-open-contract-dataset-aok-beta
```
Lo descomprimimos
```
! unzip atticus-open-contract-dataset-aok-beta.zip
```
Y con esto ya tendriamos Descargado dentro de nuestro google colab el dataset.

## ğŸ“Œ Nota: CÃ³mo Replicar el AnÃ¡lisis

Si deseas replicar los pasos y resultados obtenidos en este proyecto, simplemente sigue estos pasos:

1. **Abre el notebook en Google Colab** usando el enlace proporcionado en la secciÃ³n de Enlaces del Proyecto.
2. **Ejecuta cada bloque de cÃ³digo en orden**. Los bloques ya estÃ¡n organizados de manera que guÃ­an el flujo del proceso ETL y anÃ¡lisis de datos. Cada bloque incluye comentarios para facilitar su comprensiÃ³n y seguimiento.
3. AsegÃºrate de tener las librerÃ­as necesarias instaladas y la conexiÃ³n a Kaggle configurada segÃºn las instrucciones indicadas.
4. **Observa los resultados y grÃ¡ficos generados** despuÃ©s de ejecutar cada bloque de cÃ³digo, los cuales reflejarÃ¡n el anÃ¡lisis de los datos segÃºn el proceso ETL implementado.

De esta manera, podrÃ¡s obtener los mismos resultados y visualizaciones sin necesidad de realizar ninguna configuraciÃ³n adicional, mÃ¡s allÃ¡ de los pasos iniciales de configuraciÃ³n del entorno.

### ğŸ› ï¸ Logging Implementado

El proceso ETL incluye la funcionalidad de logging para registrar los eventos importantes del proceso, como inicio y fin del proceso, advertencias y errores. El archivo de logs se genera en `etl_process.log`.

### ğŸ“Š AnÃ¡lisis de Datos

- Se han generado grÃ¡ficos y visualizaciones para responder a las preguntas del challenge.
- Se han incluido insights adicionales basados en el anÃ¡lisis exploratorio de los datos.


## ğŸ”— Enlaces del Proyecto
Puedes acceder al notebook y explorarlo a travÃ©s de los siguientes enlaces:

-**Ver en GitHub**:
[notebook.ipynb](https://github.com/Fedesin/Challenge-ML/blob/main/Colab_challenge_meli.ipynb)
-**Abrir en Google Colab**:[Abrir en Google Colab](https://colab.research.google.com/github/Fedesin/Challenge-ML/blob/main/Colab_challenge_meli.ipynb)

### âš™ï¸ Requisitos Previos

1. **Entorno de desarrollo:**
   - Google Colab o entorno local con Python 3.7+
   - ConexiÃ³n a internet para descargar el dataset desde Kaggle.

2. **LibrerÃ­as necesarias:**
   - pandas
   - numpy
   - matplotlib
   - logging 

3. **Dataset:**
   - Descargue el dataset "Atticus Open Contract Dataset (AOK-Beta)" desde [Kaggle](https://www.kaggle.com/datasets/konradb/atticus-open-contract-dataset-aok-beta) y colÃ³quelo en la carpeta `data/` del proyecto.


### ğŸ“Œ Conclusiones
Este proyecto representÃ³ un desafÃ­o interesante y enriquecedor, ya que abordÃ© una rama en la que no habÃ­a trabajado anteriormente. A lo largo del proceso, pude aprender muchÃ­simo sobre tÃ©cnicas de ETL y anÃ¡lisis de datos aplicados a contratos legales. A pesar de ser un terreno nuevo, logrÃ© alcanzar resultados satisfactorios y obtener insights relevantes a partir del dataset. Sin dudas, fue una experiencia valiosa que me permitiÃ³ expandir mis conocimientos y habilidades en anÃ¡lisis de datos.

### â„¹ï¸ AclaraciÃ³n final
Para la realizaciÃ³n de este proyecto, se utilizaron herramientas de asistencia como ChatGPT para resolver dudas, mejorar la estructura del cÃ³digo y optimizar el anÃ¡lisis de datos. Todas las implementaciones fueron realizadas y verificadas manualmente para asegurar la comprensiÃ³n y calidad del trabajo.

Aunque este proyecto podrÃ­a haberse desarrollado en un entorno local utilizando Python, optÃ© por utilizar Google Colab. La elecciÃ³n de Colab se debiÃ³ a que, al investigar cÃ³mo descargar y procesar el dataset desde Kaggle, encontrÃ© que Colab es una herramienta muy utilizada para tareas de anÃ¡lisis de datos y machine learning. 

AdemÃ¡s, al usar Colab, no tuve que preocuparme por la configuraciÃ³n de dependencias y librerÃ­as en mi entorno local, lo que me permitiÃ³ concentrarme en el anÃ¡lisis y procesamiento del dataset de manera mÃ¡s eficiente.

